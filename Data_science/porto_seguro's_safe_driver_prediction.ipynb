{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLxC7ygZeydr7xWFcKWhvl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MinsooKwak/Kaggle_note/blob/main/Data_science/porto_seguro's_safe_driver_prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "필사 스터디 참고: \n",
        "\n",
        "https://www.kaggle.com/code/gpreda/porto-seguro-exploratory-analysis-and-prediction/notebook"
      ],
      "metadata": {
        "id": "I6_brCCAJhTa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8f8dDb5JYxK",
        "outputId": "5612423f-f3a3-4ba1-ebad-3e5839bb2057"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Ioa55IaiJt5e"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 무작위 순열 # 무작위 셔플 넣기\n",
        "from sklearn.utils import shuffle # 일관된 방식으로 배열 또는 희소 행렬을 섞는다\n",
        "from sklearn.impute import SimpleImputer\n",
        "# imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# VarianceThreshold : 변수의 variance가 threshold보다 작으면 drop\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "# SelectFromModel은 지도학습 모델로 계산된 중요도가 지정한 임계치보다 큰 모든 특성을 선택\n",
        "# 단변량과는 달리 한 번에 모든 특성을 고려해 상호작용 부분을 적용 가능\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# 교차검증 : 과적합을 방지하기 위한 방법\n",
        "# StratifiedKFold는 각 클래스에 대한 비율을 고려해 fold를 나눔\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "# 교차검증을 더 쉽게 하기 위한 API\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "metadata": {
        "id": "fFcb_gDCJ1pz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lightgbm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWvFnnfLBy7b",
        "outputId": "d6186950-f43c-4ad1-f55e-5df4901b3423"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.7/dist-packages (2.2.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.7.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from lightgbm) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->lightgbm) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lightgbm as lgb\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "pd.set_option('display.max_columns',100)"
      ],
      "metadata": {
        "id": "xCsSEsKAB1cx"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imputation 방법론 (study)"
      ],
      "metadata": {
        "id": "dZPwkfOZRRZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Drop \n",
        "  - 중요한 정보를 가진 데이터를 잃을 위험 있음\n",
        "  - 통계적 편향이 생김\n",
        "\n",
        "  => Impuation\n",
        "\n",
        "**[Single Impuation]**\n",
        "- Mean, Median Imputation\n",
        "  - 작은 데이터셋에 용이\n",
        "  - 숫자형 데이터에만 사용할 수 있음\n",
        "  - 결측치가 존재하는 컬럼만 고려\n",
        "  - 다른 feature 간의 상관관계가 고려되지 않음\n",
        "\n",
        "    ```\n",
        "    from sklearn.impute import SimpleImputer\n",
        "\n",
        "    imp_mean = SimpleImputer(strategy = 'mean') \n",
        "    # median 쓰면 중앙값\n",
        "    imp_mean.fit(df)\n",
        "    ```\n",
        "    - Mean imputation\n",
        "    - Median Imputation\n",
        "\n",
        "- Most Frequent Value Imputation\n",
        "  - 가장 빈번히 나온 값으로 대체할 수 있음\n",
        "  - 범주형 feature에도 활용할 수 있음\n",
        "\n",
        "  ```\n",
        "  from skelarn.impute import SimpleImputer\n",
        "  imp_mean =SimpleImputer(stretegy='most_frequent')\n",
        "  imp_mean.fit(df)\n",
        "  df_imputed = pd.DataFrame(imp_mean.transform(df))\n",
        "  ```\n",
        "\n",
        "- Zero Imputation / Constant Imputation\n",
        "  - 0으로 대체 / 지정한 상수값으로 대체\n",
        "  - 범주형 feature에 잘 작동함\n",
        "  - 다른 feature 간의 상관관계가 고려되지 않음\n",
        "  - 데이터에 bias(편향)을 만들 수 있음\n",
        "\n",
        "\n",
        "- K-NN Imputation\n",
        "  - feature similarity를 활용하여 가장 닮은(근접한) 데이터를 K개 찾는 방식\n",
        "  - impyute라이브러리를 활용\n",
        "  - KDTree를 생성한 후 가장 가까운 이웃(NN) 찾음\n",
        "  - K개의 NN 찾은 후 거리에 따른 가중 평균 취함\n",
        "  - mean, median, most_frequent 보다 정확\n",
        "  - 메모리가 많이 필요하며, 전체 데이터셋을 메모리에 올려야한다.\n",
        "  - 이상치에 민감하다.\n",
        "\n",
        "  ```\n",
        "  from impyute.imputation.cs import fast_knn\n",
        "  np_imputed = fast_knn(df.values, k=5) # knn 학습\n",
        "  df_imputed = pd.DataFrame(np_imputed)\n",
        "  ```\n",
        "\n",
        "\n",
        "- Stochastic regression imputation\n",
        "  - 동일 데이터셋, 관련 다른 feature에서 missing value 예측\n",
        "  - 다른 값들로 회귀 추정을 해서 imputation 함\n",
        "  - 신뢰도가 과대평가 되는 것을 방지하기 위해 residual(잔차)를 추가해줌\n",
        "  - simple impuation 중엔 가장 편향이 적은 결과(여전히 과대평가)\n",
        "\n",
        "- Extrapolation and Interpolation\n",
        "  - 이산형 범위 내 다른 데이터로 부터 값 추정\n",
        "  - interpolation\n",
        "    - 20살일때의 키, 40살일 때의 키 -> 30살일때의 키\n",
        "  - extrapolation\n",
        "    - 1살~현재 키 -> 앞으로 10년 후의 키 예측\n",
        "    - interpolation보다 안정성 떨어짐\n",
        "\n",
        "- Hot-Deck Imputation\n",
        "  - 관련된 비슷한 데이터셋에서 랜덤하게 선택\n",
        "  - 가장 많은 것을 넣거나, 안변했을 것이다\n",
        "\n",
        "**[Multiple Impuation]**\n",
        "\n",
        "=> 결측 대체값들을 만들 때마다 residual로 인한 변동으로 관측값보다 상대적으로 약한 계수 추정 신뢰도를 갖게 됨\n",
        "\n",
        "=> single impuation을 거친 여러개의 데이터셋을 만들어 평가함 \n",
        "\n",
        "\n",
        "- MICE(Multivariate Impustation by Chained Equation) Imputation\n",
        "  - 누락된 데이터를 여러 번 채움\n",
        "  - 불확실성을 고려하면 Single Imputation보다 나음\n",
        "  - chained equation은 유연하여 연속형, 이진형, 범위형, survey skip 패턴을 처리할 수 있음\n",
        "  - impuation, Analysis, Pooling\n",
        "  - distribution을 토대로 m개의 데이터셋을 imputation 함\n",
        "  - m개의 완성된 데이터셋 분석\n",
        "  - 평균, 분산, 신뢰구간을 계산해 결과를 합침\n",
        "\n",
        "  ```\n",
        "  from impyute.impuation.cs import mice\n",
        "  np_imputed = mice(df.values) # mice 학습 시작\n",
        "  df_imputed = pd.DataFrame(np_imputed)\n",
        "  ```\n",
        "\n",
        "- Deeplearning 이용한 Imputation / Datawig\n",
        "  - 범주형/ non numerical feature에 효과적\n",
        "  - DNN 통해 학습하고 누락된 값 유추\n",
        "  - 타 방식에 비해 정확\n",
        "  - CPU, GPU 지원\n",
        "  - 한 번에 한개의 컬럼만 대체 가능\n",
        "  - 대규모 데이터셋에 속도가 느림\n",
        "  - 유추하는 feature에 관련한 정보가 들어있는 feature 직접 지정해야 함\n",
        "\n",
        "\n",
        "  ```\n",
        "  import datawig\n",
        "  imputer = datawig.SimpleImputer(\n",
        "    input_columns = ['1','2','4'] # '3'컬럼 제외\n",
        "    output_column = '0' # 컬럼 0의 결측치 채움\n",
        "    imputer.fit(train_df=df, num_epochs=50)\n",
        "    df_null = df[df['0'].isnull()]\n",
        "    np_imputed = imputer.predict(df_null)\n",
        "    df_imputed = pd.DataFrame(np_imputed)\n",
        "  ```\n",
        "\n",
        "\n",
        "\n",
        "참고1: https://dining-developer.tistory.com/19\n",
        "\n",
        "참고2: https://m.blog.naver.com/hancury/220396495672"
      ],
      "metadata": {
        "id": "d2-i5tFTLYOO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 다항회귀"
      ],
      "metadata": {
        "id": "rGYeDczGRloW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터들간의 형태가 비선형일 때 현재 데이터를 다항식 형태로 변경\n",
        "\n",
        "  ```\n",
        "  from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "  poly_features= PolynominalFeatures(degree=2, include_bias=False)\n",
        "  x_poly = poly_feautures.fit_transform(x)\n",
        "  # parameter:\n",
        "    # degree : 차수 조절\n",
        "    # include_bias = True : 0차항도 함께 만듦\n",
        "  ```\n",
        "\n",
        "참고: https://inuplace.tistory.com/515"
      ],
      "metadata": {
        "id": "pgcbW9T5UM-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection"
      ],
      "metadata": {
        "id": "I82MMYyaVdzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection Method\n",
        "\n",
        "1. Filter's method\n",
        "- 단일 변수를 통계량을 이용해 평가\n",
        "- 다른 method에 비해 계산량, 시간 적게 소모\n",
        "- 다른 변수와의 상관관계를 고려하지 않으므로, 모델 성능이 떨어짐\n",
        "- variance threshold, correlation coefficient\n",
        "  - **variance threshold** : 변수의 variance가 threshold보다 작으면 drop \n",
        "    - 변수의 varience가 작으면 변수의 value가 target에 미치는 영향의 차이가 미비하기 떄문\n",
        "    - 개별 변수의 varience만 고려함(단점)\n",
        "    - 변수의 variance가 작더라도 target과의 상관관계는 높을 수 있으며 target에 영향을 줄 수 있지만 제거 (모델의 성능에 영향)\n",
        "\n",
        "    ```\n",
        "    from sklearn.feature_selection import VarienceThreshold\n",
        "    selector = VarienceThreshold(0.8)\n",
        "    train_thresh = selector.fit(train)\n",
        "    ```\n",
        "  - **correlation coefficient**\n",
        "    - Target변수, 개별 feature 상관계수 구하고 상관계수 절댓값 큰 feature 선택\n",
        "\n",
        "\n",
        "2. Wrappers' method\n",
        "- 모델을 학습하며 feature 선택하는 과정\n",
        "- Feature를 하나씩 제거하거나 더하며(모든 feature subset을 하나씩 학습하며) 가장 성능이 높게 나오는 feature set을 선택하는 방법\n",
        "- Filter's method보다는 compuation, 시간 오래 걸림(단점)\n",
        "- 모든 feature subset을 평가해 우수한 성능\n",
        "- Recursive Feature Elimination(RFE)\n",
        "  - 전체 feature를 포함하여 모델을 학습하고 이후 feature를 하나씩 제거하며 가장 성능이 좋은 feature subset을 결정하는 방법\n",
        "  - 최종 선택 개수는 주관적이므로 RRECV를 통해 cross validation을 통해 적절한 feature 수 결정함\n",
        "  \n",
        "    ```\n",
        "    rfe = RFE(rf, n_features_to_select=20)\n",
        "    train_rfe = rfe.fit(X,y)\n",
        "    # n_features_to_select : 최종적으로 선택할 feature의 수\n",
        "    # get_support() : 어떤 feature가 선택되었는지 확인 가능\n",
        "    ```\n",
        "    - #RFECV 이용 방법:\n",
        "\n",
        "    ```\n",
        "    selector = RFECV(rf, step=1, cv=3)\n",
        "    # step : 테스트 한 번 마다 몇개의 feature 제거할 것인지\n",
        "    selector.fit(X,y)\n",
        "\n",
        "    print(selector.support_) # 선택된 feature 확인 (True=선택, False=제거)\n",
        "    print(selector.ranking_) # 각 feature의 ranking 확인 (ranking=1: 선택/ 그 이외 제외)\n",
        "    ```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "3. Embedded methods\n",
        "- filter's methods + wrapper's methods\n",
        "- tree 기반: feature importance\n",
        "    - tree 기반 알고리즘을 사용하는 모델에 적용\n",
        "    - information gain과 관련 있음\n",
        "      - 상위 노드의 불순도-분기한 후 좌/우 노드의 불순도를 계산한 것\n",
        "      - information gain이 크면 노드가 분기했을 때 불순도가 많이 감소한다는 뜻\n",
        "    - 모델은 information gain이 큰 node를 선택해 그 node 기준으로 트리를 분기함\n",
        "    - 불순도를 많이 감소시킬수록 importance가 커짐\n",
        "- L1, L2 regulation\n",
        "  - L1 규제 : 일부 특성의 계수만  학습(Lasso)\n",
        "\n",
        "\n",
        "\n",
        "참고1: https://datascienceschool.net/03%20machine%20learning/14.03%20%ED%8A%B9%EC%A7%95%20%EC%84%A0%ED%83%9D.html\n",
        "\n",
        "참고2: https://hyewonleess.github.io/ml/feature_selection/\n",
        "\n",
        "SelectFromModel 참고 : https://woolulu.tistory.com/66"
      ],
      "metadata": {
        "id": "-vXyetnqXFR_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 교차검증"
      ],
      "metadata": {
        "id": "yQ_MW_A-7x4_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "한 번의 학습을 통해 평가를 하면 과적합을 할 가능성이 크다.\n",
        "\n",
        "교차검증을 통해 과적합을 막아줄 수 있다.\n",
        "\n",
        "1. 데이터가 독립적이고 동일한 분포\n",
        "  - KFold, RepeatedKFold, LeaveOneOut(LOO), LeaveOutLeaveOneOut(LPO)\n",
        "2. 데이터가 동일한 분포가 아닐 때\n",
        "  - StratifiedKFold,RepeatedStratifiedKFold, StratifiedShuffleSplit\n",
        "3. 그룹화된 데이터의 경우\n",
        "  - GroupKFold, LeaveOneGroupOut, LeavePGroupsOut, GroupShuffleSplit\n",
        "4. 시계열 데이터\n",
        "  - TimeSeriesSplit\n",
        "\n",
        "---\n",
        "\n",
        "- **K-Fold** : k 값 만큼의 폴드 세트에 k번의 학습과 검증을 진행함\n",
        "  - 일정한 간격으로 잘라서 사용\n",
        "  - class imbalance를 고려하지 않음\n",
        "\n",
        "  참고: https://continuous-development.tistory.com/166\n",
        "\n",
        "- **StratifiedKFold** : \n",
        "  - 레이블 값의 분포를 반영\n",
        "  - KFold과 같이 데이터가 한 곳으로 몰리는 것을 방지 (class imbalance 고려)\n",
        "\n",
        "- 교차검증 쉽게 하기 위한 API : cross_val_score()\n",
        "  - cross_val_score(estimator, X, y, scoring=평가지표, cv=교차 검증 폴드 수)\n",
        "    - estimator == classifier종류\n",
        "      - 내부적으로 stratifiedKFold 진행\n",
        "    - cv 지정 횟수만큼 평가지표로 평가 결과값 배열로 반환\n",
        "      - 평균하여 평가수치로 일반적으로 활용\n",
        "\n",
        "참고: https://davinci-ai.tistory.com/18\n"
      ],
      "metadata": {
        "id": "YEbyIfFK73GE"
      }
    }
  ]
}